{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train gcn model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from spektral.data import Dataset, DisjointLoader, Graph\n",
    "from spektral.layers import GCSConv, GCNConv, GeneralConv\n",
    "from spektral.layers import GlobalAvgPool, GlobalMaxPool\n",
    "\n",
    "# load data\n",
    "# adj_filename = 'reduced_X_correlation_epilepsy_corpus_60s.npy'\n",
    "adj_filename = 'reduced_X_normed_mi_epilepsy_corpus_60s.npy'\n",
    "\n",
    "reduced_index_df = pd.read_csv(\"reduced_epilepsy_corpus_window_index_60s.csv\")\n",
    "chose_ids = reduced_index_df[['patient_id', 'numeric_label']].drop_duplicates()\n",
    "\n",
    "train_ids = chose_ids['patient_id'].sample(frac=0.6, random_state=3).tolist() \n",
    "val_ids = (chose_ids[-chose_ids['patient_id'].isin(train_ids)]['patient_id'].\n",
    "            sample(frac=0.5, random_state=4).tolist())\n",
    "test_ids = (chose_ids[-chose_ids['patient_id'].\n",
    "            isin(train_ids+val_ids)]['patient_id'].tolist())\n",
    "\n",
    "class LoadGraphs(Dataset):\n",
    "    def __init__(self, patient_ids, **kwargs):\n",
    "        self.patient_ids = patient_ids\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self): \n",
    "        reduced_index_df = pd.read_csv(\"reduced_epilepsy_corpus_window_index_60s.csv\")\n",
    "        idx = (reduced_index_df[reduced_index_df['patient_id'].\n",
    "               isin(self.patient_ids)].index.values)\n",
    "        adj_60s = np.load(adj_filename) \n",
    "        adj_60s = adj_60s[idx]\n",
    "        y_60s = np.load('reduced_y_epilepsy_corpus_60s.npy', allow_pickle=True)\n",
    "        y_60s = y_60s[idx]\n",
    "        eeg_60s = np.load('reduced_X_windows_epilepsy_corpus_60s.npy', allow_pickle=True)\n",
    "        eeg_60s = eeg_60s[idx]\n",
    "        \n",
    "        y = [0 if 'no_epilepsy'==v else 1 for v in y_60s]\n",
    "        \n",
    "        # for categorical cross-entropy\n",
    "        y_col = np.zeros((len(y),2),)\n",
    "        for i,x in enumerate(y):\n",
    "            y_col[i,x] = 1\n",
    "        \n",
    "        A = adj_60s.reshape(len(y), 19,19)\n",
    "\n",
    "        data_gnn = []\n",
    "        for x,a,label in zip(eeg_60s, A, y_col):\n",
    "            proportional_thresholding(matrix_a=a, percentile=50)\n",
    "            data_gnn.append(Graph(x=x, a=sp.csr_matrix(a), y=label))\n",
    "    \n",
    "        del adj_60s, A, y, y_60s, y_col, eeg_60s  \n",
    "        return data_gnn\n",
    "\n",
    "\n",
    "# reading preprocessed data\n",
    "data_tr = LoadGraphs(patient_ids=train_ids)\n",
    "data_va = LoadGraphs(patient_ids=train_ids)\n",
    "data_te = LoadGraphs(patient_ids=test_ids)\n",
    "\n",
    "# shuffle graph datasets\n",
    "np.random.seed(1)\n",
    "idx_tr = np.random.permutation(len(data_tr))\n",
    "data_tr = data_tr[idx_tr]\n",
    "idx_va = np.random.permutation(len(data_va))\n",
    "data_va = data_va[idx_va]\n",
    "idx_te = np.random.permutation(len(data_te))\n",
    "data_te = data_te[idx_te]\n",
    "\n",
    "# config\n",
    "learning_rate = 1e-4  # learning rate\n",
    "epochs = 80  # number of training epochs\n",
    "es_patience = 10  # patience for early stopping\n",
    "batch_size = 32  # batch size\n",
    "\n",
    "# data loaders\n",
    "loader_tr = DisjointLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = DisjointLoader(data_va, batch_size=batch_size)\n",
    "loader_te = DisjointLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# build model\n",
    "network_prarams={'channels': 32,\n",
    "            'activation': 'relu', \n",
    "            'aggregate': 'mean',\n",
    "            'dropout': 0.25,\n",
    "            'kernel_regularizer':'l2',\n",
    "            'bias_regularizer':'l2',\n",
    "            'activity_regularizer':'l2'\n",
    "            }\n",
    "class Net(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GeneralConv(**network_prarams)\n",
    "        self.conv2 = GeneralConv(**network_prarams)\n",
    "        self.conv3 = GeneralConv(**network_prarams) \n",
    "        self.global_pool = GlobalMaxPool()\n",
    "        self.dense1 = Dense(19, activation=\"relu\")\n",
    "        self.dense2 = Dense(4, activation=\"relu\")\n",
    "        self.dense3 = Dense(2, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, i = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        x = self.conv3([x, a])\n",
    "        output = self.global_pool([x, i])\n",
    "        output = self.dense1(output)\n",
    "        output = self.dense2(output)\n",
    "        output = self.dense3(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "model = Net()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "\n",
    "# fit the model\n",
    "@tf.function(input_signature=loader_tr.tf_signature(),\n",
    "             experimental_relax_shapes=True)\n",
    "\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "    return loss, acc\n",
    "\n",
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  \n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n",
    "\n",
    "epoch = step = 0\n",
    "best_val_loss = np.inf\n",
    "best_weights = None\n",
    "patience = es_patience\n",
    "results = []\n",
    "tr_results = []\n",
    "val_results = []\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "    loss, acc = train_step(*batch)\n",
    "    results.append((np.array(loss), np.array(acc)))\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # compute validation loss and accuracy\n",
    "        val_loss, val_acc = evaluate(loader_va)\n",
    "        val_results.append((val_loss, val_acc))\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val loss: {:.3f} - Val acc: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), val_loss, val_acc\n",
    "            )\n",
    "        )\n",
    "        tr_results.append(np.mean(results, 0))\n",
    "\n",
    "        # check if loss improved for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = es_patience\n",
    "            print(\"New best val_loss {:.3f}\".format(val_loss))\n",
    "            best_weights = model.get_weights()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping (best val_loss: {})\".format(best_val_loss))\n",
    "                break\n",
    "        results = []\n",
    "\n",
    "# evaluate model\n",
    "model.set_weights(best_weights)  # load best model\n",
    "test_loss, test_acc = evaluate(loader_te)\n",
    "print(\"Done. Test loss: {:.4f}. Test acc: {:.2f}\".format(test_loss, test_acc))\n",
    "\n",
    "# plot evaluation results\n",
    "val_results = np.array(val_results)\n",
    "tr_results = np.array(tr_results)\n",
    "  \n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 8))\n",
    "ax.plot(tr_results[:,1], lw=4, ls='-', c='b')\n",
    "ax.plot(val_results[:,1], lw=4, ls='--', c='r')\n",
    "ax.set_ylabel('Accuracy', fontsize=18)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=18)\n",
    "ax.legend([\"Train\", \"Validation\"], loc=4, fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
