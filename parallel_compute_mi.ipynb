{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from glob2 import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # dataframes, tables \n",
    "import seaborn as sns # plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import mne\n",
    "from mne.datasets import eegbci\n",
    "from mne.io import concatenate_raws, read_raw_edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob2 import glob\n",
    "\n",
    "# reading the list of file names in the EEG Epilepsy Corpus\n",
    "edf_file_list = glob(\"../tuh_eeg_epilepsy/edf/*/*/*/*/*/*.edf\")\n",
    "\n",
    "\n",
    "#edf_file_list = glob(\"../tuh_eeg_abnormal/edf/*/*/*/*/*/*/*.edf\")\n",
    "len(edf_file_list)\n",
    "\n",
    "unique_epilepsy_patient_ids = list(set([x.split(\"/\")[-1].split(\"_\")[0] for x in edf_file_list]))\n",
    "len(unique_epilepsy_patient_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_edf_to_pandas(edf_filename, select_channels = True):\n",
    "    \"\"\" Reads data from an edf file to a Pandas dataframe.\n",
    "        Column names are 'channel_labels'.\n",
    "        \n",
    "        If 'select_channels=True', then only 19 common channels are selected to \n",
    "        create the resulting dataframe. The channel names will be updated (standardized).\n",
    "        \n",
    "        Returns: dataframe, channel labels\n",
    "    \"\"\"\n",
    "    # read edf file\n",
    "    raw_data = read_raw_edf(edf_filename, verbose=False, preload=False)\n",
    "    \n",
    "    if select_channels:\n",
    "        # the TUEP database has 3 EEG channel configurations: '02_tcp_le', '03_tcp_ar_a', '01_tcp_ar'\n",
    "        # number of channels and channel names differ within these configurations\n",
    "        # to be able to compare the different EEG readings we need to select channels\n",
    "        # that are common for all configurations\n",
    "\n",
    "        # the list of 19 channels (their short labels) that we will use for analysing EEG data\n",
    "        channels_to_use = ['FP1', 'FP2', 'F7', 'F3', 'FZ', 'F4', 'F8',\n",
    "                           'T3', 'C3', 'CZ', 'C4', 'T4', 'T5',\n",
    "                           'P3', 'PZ', 'P4', 'T6', 'O1', 'O2']\n",
    "        \n",
    "        # the function to update channel names from original to new format:\n",
    "        ch_name_update_func = lambda ch: ch.split(' ')[-1].split('-')[0]\n",
    "\n",
    "        # renaming the original channel names in one .edf file;\n",
    "        # the update will be written into the in-memory edf object\n",
    "        raw_data.rename_channels(mapping=ch_name_update_func)\n",
    "        \n",
    "        # check if all required channels are in the edf file\n",
    "        try:\n",
    "            assert all([ch in raw_data.info[\"ch_names\"] for ch in channels_to_use])\n",
    "        except:\n",
    "            print('Not all required channels are in the edf file.')\n",
    "        \n",
    "        # dataframe with EEG readings from selected channels and with \n",
    "        # updated channel names\n",
    "        df = pd.DataFrame(raw_data.pick_channels(channels_to_use).get_data().T,\n",
    "            columns=raw_data.pick_channels(channels_to_use).info['ch_names'])\n",
    "        \n",
    "        # we need to return correct channel/column names\n",
    "        channel_labels = channels_to_use # as specified by us: left-to-right and top-down\n",
    "        # channel_labels = df.columns.tolist() # as given in the edf file\n",
    "        \n",
    "    else:\n",
    "        # get channel names from edf file\n",
    "        channel_labels = raw_data.info[\"ch_names\"]\n",
    "\n",
    "        # create a dataframe from\n",
    "        df = pd.DataFrame(raw_data.get_data().T, columns=channel_labels)\n",
    "\n",
    "    return df[channel_labels], channel_labels # as specified by us: left-to-right and top-down\n",
    "    # return df, channel_labels # as given in the edf file\n",
    "\n",
    "# !!! NEED TO DECIDE !!!\n",
    "# what order of channels should be given in the dataframe?\n",
    "#      - as specified in the edf file\n",
    "#      - as chosen by us: left-to-right and top-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corr_matrix(edf_filename):\n",
    "    \n",
    "    # read edf file from filename\n",
    "    # by default, common channels will be selected and renamed\n",
    "    df, channel_labels = read_edf_to_pandas(edf_filename)\n",
    "    \n",
    "    # calculate the correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "\n",
    "def plot_correlation_matrix(edf_filename):\n",
    "    \"\"\" Reads edf file from relative path (ex. ../tuh_eeg_epilepsy/edf/*/*/*/*/*/*.edf). \n",
    "        Creates a dataframe with all EEG readings from all channels.\n",
    "        Computes a correlation matrix.\n",
    "    \"\"\"\n",
    "    # read edf file from filename\n",
    "    # by default, common channels will be selected and renamed\n",
    "    df, channel_labels = read_edf_to_pandas(edf_filename)\n",
    "    \n",
    "    # calculate the correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "        \n",
    "    # plot the heatmap for correlation matrix\n",
    "    fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "    sns.heatmap(corr_matrix,\n",
    "                xticklabels=channel_labels, \n",
    "                yticklabels=channel_labels,\n",
    "                cmap= plt.cm.jet,\n",
    "                ax = ax)\n",
    "    \n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.xlabel('channel_names')\n",
    "    plt.ylabel('channel_names')\n",
    "     \n",
    "    # return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(bins, *X):\n",
    "    \n",
    "    # binning of the data\n",
    "    data, *edges = np.histogramdd(X, bins=bins)\n",
    "    \n",
    "    # calculate probabilities\n",
    "    data = data.astype(float)/data.sum()\n",
    "    \n",
    "    # compute H(X,Y,...,Z) = sum(-P(x,y,...,z) ∗ log2(P(x,y,...,z)))\n",
    "    return np.sum(-data * np.log2(data+sys.float_info.epsilon))\n",
    "\n",
    "\n",
    "def mutual_information(bins, X, Y):\n",
    "    \n",
    "    # compute I(X,Y) = H(X) + H(Y) − H(X,Y)\n",
    "    \n",
    "    H_X = entropy(bins, X)\n",
    "    H_Y = entropy(bins, Y)\n",
    "    H_XY = entropy(bins, X, Y)\n",
    "    \n",
    "    return H_X + H_Y - H_XY\n",
    "\n",
    "# Compute number of bins using Sturge's rule\n",
    "def compute_mi_matrix(df):\n",
    "    \"\"\" Compute Mutual Information matrix.\n",
    "    \n",
    "        Return: mi_matrix\n",
    "    \"\"\"\n",
    "    n_cols = df.shape[1]\n",
    "    mi_matrix = np.zeros([n_cols, n_cols])\n",
    "    \n",
    "    # Sturge's rule for number of bins\n",
    "    n_bins = int(1 + 3.322*np.log2(df.shape[0]))\n",
    "    \n",
    "    for i in range(n_cols):\n",
    "        for j in range(n_cols):\n",
    "            mi = mutual_information(n_bins, df.iloc[:,i],df.iloc[:,j])\n",
    "            mi_matrix[i,j] = mi\n",
    "    \n",
    "    return mi_matrix\n",
    "    \n",
    "\n",
    "def compute_normed_mi_matrix(mi_matrix):\n",
    "    \"\"\" Compute normalized version of the given Mutual Information matrix.\n",
    "    \n",
    "        Return: normed_mi_matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # normalize mi matrix by dividing matrix elements with\n",
    "    # sqrt of product of respective diagonal elements\n",
    "    divisor_matrix = np.sqrt(np.diag(mi_matrix)*np.diag(mi_matrix).reshape(-1,1))\n",
    "    normed_mi_matrix = mi_matrix/divisor_matrix\n",
    "\n",
    "    return normed_mi_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compute MI matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with overview over filenames in the TUEP\n",
    "# there are 1648 files in the TUEP\n",
    "\n",
    "df_files_overview = pd.DataFrame([f.split('/')[3:]+[f] for f in edf_file_list], \n",
    "            columns = ['text_label', 'ch_conf', '-', 'patient_id', 'session_date', 'filename', 'file_path'])\n",
    "\n",
    "# create some additional columns\n",
    "\n",
    "df_files_overview['session'] = df_files_overview['session_date'].apply(lambda s: s.split('_')[0])\n",
    "df_files_overview['year'] = df_files_overview['session_date'].apply(lambda s: s.split('_')[1])\n",
    "df_files_overview['month'] = df_files_overview['session_date'].apply(lambda s: s.split('_')[2])\n",
    "df_files_overview['numeric_label'] = df_files_overview['text_label'].replace(to_replace=['epilepsy', 'no_epilepsy'], value=[1,0])\n",
    "df_files_overview['token'] = df_files_overview['filename'].apply(lambda s: s.split('_')[-1][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_files_per_patient = [df_files_overview.query(f\"patient_id == '{id}'\")['file_path'].values[0] for id in unique_epilepsy_patient_ids]\n",
    "df_files_per_patient = df_files_overview[df_files_overview['file_path'].isin(edf_files_per_patient)]\n",
    "chosen_files=[f for f in edf_files_per_patient if '/epilepsy/' in f] + [f for f in edf_files_per_patient if 'no_epilepsy' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [20:25<00:00,  6.13s/it] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from compute_mi_matrices import compute_mi_form_edf_file\n",
    "\n",
    "frames_list = chosen_files\n",
    "\n",
    "max_pool = 5\n",
    "\n",
    "# with Pool(max_pool) as p:\n",
    "    pool_outputs = list(\n",
    "        tqdm(\n",
    "            p.imap(compute_mi_form_edf_file,\n",
    "                   frames_list),\n",
    "            total=len(frames_list)\n",
    "        )\n",
    "    )    \n",
    "\n",
    "# print(pool_outputs)\n",
    "new_dict = dict(pool_outputs)\n",
    "# print(\"dict:\", new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the obtained dictionary into a file using joblib\n",
    "\n",
    "import joblib\n",
    "\n",
    "# joblib.dump(new_dict, './matrices/mi_matrices_v0.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = joblib.load('./matrices/mi_matrices_v0.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [3:51:55<00:00, 69.58s/it]    \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from compute_mi_for_bands import compute_mi_over_bands_form_edf_file\n",
    "\n",
    "frames_list = chosen_files\n",
    "\n",
    "max_pool = 5\n",
    "\n",
    "with Pool(max_pool) as p:\n",
    "    pool_outputs = list(\n",
    "        tqdm(\n",
    "            p.imap(compute_mi_over_bands_form_edf_file,\n",
    "                   frames_list),\n",
    "            total=len(frames_list)\n",
    "        )\n",
    "    )    \n",
    "\n",
    "# print(pool_outputs)\n",
    "new_dict = dict(pool_outputs)\n",
    "# print(\"dict:\", new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./matrices/mi_matrices_for_bands.data']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the obtained dictionary into a file using joblib\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(new_dict, './matrices/mi_matrices_for_bands.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "042e8f1b8e63e032bef6e8ea3df418ddbf2c1d06d8367452e5884c8c7f8eef50"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
